"""
é¢˜ç›®å¹¶è¡Œç”ŸæˆæœåŠ¡ - æ”¯æŒå¤šæ¥æºå‡ºé¢˜

ä½¿ç”¨ LangChain + Pydantic ç»“æ„åŒ–è¾“å‡ºï¼Œç¡®ä¿ AI ç”Ÿæˆå†…å®¹å¯è¿½æº¯ã€å¯æ£€ç´¢ã€‚

ç­–ç•¥ï¼š
- å®˜æ–¹ API (system mode): ä½¿ç”¨ä¸¤é˜¶æ®µå¹¶è¡Œæ¶æ„
  - é˜¶æ®µ1: è§„åˆ’é¢˜ç›®åˆ†å¸ƒï¼ˆé¢˜å‹ã€éš¾åº¦ã€è€ƒç‚¹ï¼‰
  - é˜¶æ®µ2: å¹¶å‘ç”Ÿæˆæ¯é“é¢˜ç›®
- ç”¨æˆ·è‡ªé… API (byok mode): ä½¿ç”¨å•æ¬¡ç”Ÿæˆæ–¹å¼
"""
import asyncio
import json
import re
import time
import uuid
from datetime import datetime
from typing import AsyncGenerator, Literal

import httpx
from pydantic import BaseModel, ConfigDict, Field
from sqlalchemy.orm import Session

from app.services.llm_context import resolve_llm_config
from app.services import credits as credits_service
from app.services.llm import new_request_id

# é¢˜ç›®ç”Ÿæˆå›ºå®šç§¯åˆ†æ¶ˆè€—ï¼ˆæ¯é“é¢˜ï¼‰
QUESTION_GENERATION_POINTS_PER_QUESTION = 5


# ==================== AI å…ƒæ•°æ® Schemaï¼ˆç”¨äºè¿½æº¯å’Œæ£€ç´¢ï¼‰====================

class AIGenerationMeta(BaseModel):
    """AI ç”Ÿæˆå…ƒæ•°æ® - è®°å½•ç”Ÿæˆè¿‡ç¨‹ä¿¡æ¯ï¼Œä¾¿äºè¿½æº¯å’Œåˆ†æ"""
    generated_at: str = Field(
        default_factory=lambda: datetime.now().isoformat(),
        alias="generatedAt",
        description="ç”Ÿæˆæ—¶é—´ ISO æ ¼å¼"
    )
    model_id: str = Field(default="", alias="modelId", description="ä½¿ç”¨çš„æ¨¡å‹ ID")
    strategy: str = Field(default="parallel", description="ç”Ÿæˆç­–ç•¥: parallel / single")
    api_mode: str = Field(default="system", alias="apiMode", description="API æ¨¡å¼: system / byok")
    prompt_version: str = Field(default="v3.0", alias="promptVersion", description="Prompt ç‰ˆæœ¬å·")
    generation_id: str = Field(
        default_factory=lambda: str(uuid.uuid4())[:8],
        alias="generationId",
        description="æœ¬æ¬¡ç”Ÿæˆçš„å”¯ä¸€æ ‡è¯†"
    )

    model_config = ConfigDict(populate_by_name=True, serialize_by_alias=True)


class AIContentTrace(BaseModel):
    """AI å†…å®¹è¿½æº¯ä¿¡æ¯ - è®°å½•å†…å®¹æ¥æºå’Œç”Ÿæˆä¾æ®"""
    source_type: str = Field(
        default="node_content", alias="sourceType",
        description="æ¥æºç±»å‹: node_content / user_material / learning_goals / mixed"
    )
    source_id: str = Field(default="", alias="sourceId", description="æ¥æº ID")
    source_name: str = Field(default="", alias="sourceName", description="æ¥æºåç§°")
    source_context: str = Field(default="", alias="sourceContext", description="å‡ºé¢˜ä¾æ®çš„ä¸Šä¸‹æ–‡æ‘˜è¦")
    confidence: float = Field(default=0.8, description="AI å¯¹è¯¥å†…å®¹çš„ç½®ä¿¡åº¦ 0-1")

    model_config = ConfigDict(populate_by_name=True, serialize_by_alias=True)


class AISearchableTags(BaseModel):
    """AI å¯æ£€ç´¢æ ‡ç­¾ - ç”¨äºåç»­æ™ºèƒ½æ£€ç´¢å’Œæ¨è"""
    concepts: list[str] = Field(default_factory=list, description="æ¶‰åŠçš„æ ¸å¿ƒæ¦‚å¿µ")
    skills: list[str] = Field(default_factory=list, description="è€ƒå¯Ÿçš„èƒ½åŠ›/æŠ€èƒ½æ ‡ç­¾")
    bloom_level: str = Field(
        default="understand", alias="bloomLevel",
        description="å¸ƒé²å§†è®¤çŸ¥å±‚æ¬¡: remember/understand/apply/analyze/evaluate/create"
    )
    keywords: list[str] = Field(default_factory=list, description="å…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºå…¨æ–‡æ£€ç´¢")
    similar_question_hints: list[str] = Field(
        default_factory=list, alias="similarQuestionHints",
        description="ç›¸ä¼¼é¢˜ç›®çš„å‡ºé¢˜æ–¹å‘æç¤º"
    )

    model_config = ConfigDict(populate_by_name=True, serialize_by_alias=True)


# ==================== é¢˜ç›® Schema ====================

class QuestionSchema(BaseModel):
    """
    å•é“é¢˜ç›®ç»“æ„ - åŒ…å«å®Œæ•´çš„ AI ç”Ÿæˆå…ƒæ•°æ®

    è®¾è®¡åŸåˆ™ï¼š
    1. æ‰€æœ‰ AI ç”Ÿæˆçš„å†…å®¹éƒ½ç•™ä¸‹å¯è¿½æº¯çš„å…ƒæ•°æ®
    2. ä¾¿äºåç»­æ™ºèƒ½æ£€ç´¢ã€æ¨èã€å˜å¼é¢˜ç”Ÿæˆ
    3. ä½¿ç”¨ Pydantic ç¡®ä¿è¾“å‡ºæ ¼å¼ç¨³å®š
    """
    # åŸºç¡€å­—æ®µ
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    type: Literal["single", "multiple", "judge"] = "single"
    difficulty: Literal["easy", "medium", "hard"] = "medium"
    content: str = Field(description="é¢˜ç›®å†…å®¹ï¼Œæ”¯æŒ LaTeX å…¬å¼")
    options: list[str] = Field(default_factory=list, description="é€‰é¡¹åˆ—è¡¨")
    answer: str | list[str] = Field(default="", description="æ­£ç¡®ç­”æ¡ˆ")
    explanation: str = Field(default="", description="ç­”æ¡ˆè§£æ")

    # AI å†…å®¹è¿½æº¯
    trace: AIContentTrace = Field(
        default_factory=AIContentTrace,
        description="å†…å®¹æ¥æºè¿½æº¯ä¿¡æ¯"
    )

    # AI å¯æ£€ç´¢æ ‡ç­¾
    tags: AISearchableTags = Field(
        default_factory=AISearchableTags,
        description="å¯æ£€ç´¢æ ‡ç­¾ï¼Œç”¨äºæ™ºèƒ½æ¨è"
    )

    # AI ç”Ÿæˆå…ƒæ•°æ®
    ai_meta: AIGenerationMeta = Field(
        default_factory=AIGenerationMeta,
        alias="aiMeta",
        description="AI ç”Ÿæˆè¿‡ç¨‹å…ƒæ•°æ®"
    )

    # éš¾åº¦åˆ†æ
    difficulty_reason: str = Field(default="", alias="difficultyReason", description="éš¾åº¦åˆ¤å®šåŸå› ")

    # å­¦ä¹ ç›®æ ‡å…³è”
    target_goal_id: str | None = Field(None, alias="targetGoalId", description="å…³è”çš„å­¦ä¹ ç›®æ ‡ ID")
    target_goal_name: str | None = Field(None, alias="targetGoalName", description="å…³è”çš„å­¦ä¹ ç›®æ ‡åç§°")

    # å˜å¼é¢˜æç¤ºï¼ˆç”¨äºåç»­ç”Ÿæˆç›¸ä¼¼é¢˜ï¼‰
    variation_hints: list[str] = Field(
        default_factory=list,
        alias="variationHints",
        description="å˜å¼é¢˜ç”Ÿæˆæç¤º"
    )

    model_config = ConfigDict(populate_by_name=True, serialize_by_alias=True)


class QuestionPlanItem(BaseModel):
    """é¢˜ç›®è§„åˆ’é¡¹"""
    index: int
    type: Literal["single", "multiple", "judge"] = "single"
    difficulty: Literal["easy", "medium", "hard"] = "medium"
    focus_point: str = Field(alias="focusPoint")  # è€ƒå¯Ÿé‡ç‚¹
    question_direction: str = Field(alias="questionDirection")  # å‡ºé¢˜æ–¹å‘
    source_type: str = Field("node_content", alias="sourceType")  # æ¥æºç±»å‹
    source_name: str = Field("", alias="sourceName")  # æ¥æºåç§°

    model_config = ConfigDict(populate_by_name=True)


class MaterialInfo(BaseModel):
    """èµ„æ–™ä¿¡æ¯"""
    id: str
    name: str
    content: str = ""
    content_digest: str = Field("", alias="contentDigest")
    key_topics: list[str] = Field(default_factory=list, alias="keyTopics")
    # ç»“æ„åŒ–å†…å®¹
    structured_summary: str = Field("", alias="structuredSummary")
    structured_key_points: list[dict] = Field(default_factory=list, alias="structuredKeyPoints")
    questionable_points: list[str] = Field(default_factory=list, alias="questionablePoints")

    model_config = ConfigDict(populate_by_name=True)


class LearningGoalInfo(BaseModel):
    """å­¦ä¹ ç›®æ ‡ä¿¡æ¯"""
    id: str
    goal: str
    importance: str = "medium"  # high/medium/low
    sub_goals: list[str] = Field(default_factory=list, alias="subGoals")

    model_config = ConfigDict(populate_by_name=True)


class QuestionSourceConfig(BaseModel):
    """å‡ºé¢˜æ¥æºé…ç½®"""
    use_node_content: bool = Field(True, alias="useNodeContent")
    use_materials: bool = Field(True, alias="useMaterials")
    use_learning_goals: bool = Field(False, alias="useLearningGoals")
    selected_material_ids: list[str] = Field(default_factory=list, alias="selectedMaterialIds")
    selected_goal_ids: list[str] = Field(default_factory=list, alias="selectedGoalIds")

    model_config = ConfigDict(populate_by_name=True)


class DifficultyDistribution(BaseModel):
    """éš¾åº¦åˆ†å¸ƒé…ç½®"""
    easy: int = 30
    medium: int = 50
    hard: int = 20


class QuestionBatchRequest(BaseModel):
    """æ‰¹é‡ç”Ÿæˆè¯·æ±‚ - æ”¯æŒå¤šæ¥æº"""
    # èŠ‚ç‚¹ä¿¡æ¯
    node_id: str = Field(alias="nodeId")
    node_name: str = Field(alias="nodeName")
    node_description: str = Field("", alias="nodeDescription")
    knowledge_type: str = Field("concept", alias="knowledgeType")
    node_difficulty: str = Field("beginner", alias="nodeDifficulty")
    learning_objectives: list[str] = Field(default_factory=list, alias="learningObjectives")
    key_concepts: list[str] = Field(default_factory=list, alias="keyConcepts")
    common_mistakes: list[str] = Field(default_factory=list, alias="commonMistakes")

    # èµ„æ–™åˆ—è¡¨
    materials: list[MaterialInfo] = Field(default_factory=list)

    # å­¦ä¹ ç›®æ ‡åˆ—è¡¨
    learning_goals: list[LearningGoalInfo] = Field(default_factory=list, alias="learningGoals")

    # å‡ºé¢˜é…ç½®
    sources: QuestionSourceConfig = Field(default_factory=QuestionSourceConfig)
    difficulty: DifficultyDistribution = Field(default_factory=DifficultyDistribution)
    count: int = 5
    question_types: list[str] = Field(default_factory=lambda: ["single", "judge"], alias="questionTypes")
    mode: str = "normal"  # normal / review / advance

    # ğŸ¯ ç”¨æˆ·å­¦ä¹ ç”»åƒï¼ˆç”¨äºä¸ªæ€§åŒ–å‡ºé¢˜ï¼‰
    learner_profile_prompt: str = Field("", alias="learnerProfilePrompt")

    # å…¼å®¹æ—§å­—æ®µ
    materials_content: str = Field("", alias="materialsContent")

    model_config = ConfigDict(populate_by_name=True)


class QuestionProgressEvent(BaseModel):
    """è¿›åº¦äº‹ä»¶"""
    stage: str  # planning / generating / done
    progress: float  # 0-100
    message: str
    current_question: int = 0
    total_questions: int = 0
    completed_questions: int = 0


class QuestionGenerationResult(BaseModel):
    """é¢˜ç›®ç”Ÿæˆç»“æœ"""
    success: bool
    questions: list[QuestionSchema] = Field(default_factory=list)
    error: str | None = None
    mode: Literal["system", "byok"]
    strategy: Literal["parallel", "single_request"]
    processing_time_ms: float
    model_used: str


# ==================== LLM Client ====================

class LLMClient:
    """ç»Ÿä¸€çš„ LLM è°ƒç”¨å®¢æˆ·ç«¯"""

    def __init__(self, api_key: str, base_url: str):
        self.api_key = api_key
        self.base_url = base_url.rstrip("/")
        if "chat/completions" not in self.base_url:
            self.chat_url = f"{self.base_url}/chat/completions"
        else:
            self.chat_url = self.base_url

    async def chat(
        self,
        model: str,
        messages: list[dict],
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> str:
        timeout = httpx.Timeout(120.0, connect=10.0)

        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }

        async with httpx.AsyncClient(timeout=timeout) as client:
            resp = await client.post(
                self.chat_url,
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.api_key}",
                },
                json=payload,
            )

        if resp.status_code >= 400:
            raise Exception(f"LLM API è¯·æ±‚å¤±è´¥ ({resp.status_code}): {resp.text}")

        data = resp.json()
        return data.get("choices", [{}])[0].get("message", {}).get("content", "")


def _extract_json(content: str) -> dict | list | None:
    """ä» LLM å“åº”ä¸­æå– JSON"""
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass

    # å°è¯•æå– markdown ä»£ç å—ä¸­çš„ JSON
    match = re.search(r"```(?:json)?\s*([\s\S]*?)```", content)
    if match:
        try:
            return json.loads(match.group(1).strip())
        except json.JSONDecodeError:
            pass

    # å°è¯•æå–è£¸ JSON å¯¹è±¡æˆ–æ•°ç»„
    for pattern in [r"\[[\s\S]*\]", r"\{[\s\S]*\}"]:
        match = re.search(pattern, content)
        if match:
            try:
                return json.loads(match.group(0))
            except json.JSONDecodeError:
                pass

    return None


# ==================== ä¸Šä¸‹æ–‡æ„å»ºè¾…åŠ©å‡½æ•° ====================

def _build_context_from_request(request: QuestionBatchRequest) -> tuple[str, list[dict]]:
    """
    æ ¹æ®è¯·æ±‚é…ç½®æ„å»ºå‡ºé¢˜ä¸Šä¸‹æ–‡

    Returns:
        (context_text, source_info_list)
    """
    context_parts: list[str] = []
    source_info: list[dict] = []

    # 1. èŠ‚ç‚¹çŸ¥è¯†å†…å®¹
    if request.sources.use_node_content:
        node_context = f"""## èŠ‚ç‚¹çŸ¥è¯†ï¼š{request.node_name}
{request.node_description or 'æ— æè¿°'}

çŸ¥è¯†ç±»å‹ï¼š{request.knowledge_type}
éš¾åº¦ç­‰çº§ï¼š{request.node_difficulty}
å­¦ä¹ ç›®æ ‡ï¼š{', '.join(request.learning_objectives) or 'æ— '}
å…³é”®æ¦‚å¿µï¼š{', '.join(request.key_concepts) or 'æ— '}
å¸¸è§é”™è¯¯ï¼š{', '.join(request.common_mistakes) or 'æ— '}"""
        context_parts.append(node_context)
        source_info.append({"type": "node_content", "name": request.node_name})

    # 2. ä¸Šä¼ çš„èµ„æ–™
    if request.sources.use_materials and request.materials:
        selected_ids = set(request.sources.selected_material_ids) if request.sources.selected_material_ids else None
        selected_materials = [m for m in request.materials if selected_ids is None or m.id in selected_ids]

        for mat in selected_materials:
            if mat.structured_key_points:
                # ä½¿ç”¨ç»“æ„åŒ–å†…å®¹
                key_points_text = "\n".join([
                    f"- {kp.get('title', '')}: {kp.get('content', '')}"
                    for kp in mat.structured_key_points[:5]
                ])
                mat_content = f"""## èµ„æ–™ï¼š{mat.name}
æ‘˜è¦ï¼š{mat.structured_summary or mat.content_digest}

å…³é”®çŸ¥è¯†ç‚¹ï¼š
{key_points_text}

å¯å‡ºé¢˜ç‚¹ï¼š{', '.join(mat.questionable_points[:5]) if mat.questionable_points else 'æ— '}"""
            else:
                # ä½¿ç”¨åŸå§‹å†…å®¹
                mat_content = f"""## èµ„æ–™ï¼š{mat.name}
{mat.content_digest or mat.content[:1500] if mat.content else 'æ— å†…å®¹'}"""

            context_parts.append(mat_content)
            source_info.append({"type": "user_material", "name": mat.name, "id": mat.id})

    # 3. å­¦ä¹ ç›®æ ‡
    if request.sources.use_learning_goals and request.learning_goals:
        selected_ids = set(request.sources.selected_goal_ids) if request.sources.selected_goal_ids else None
        selected_goals = [g for g in request.learning_goals if selected_ids is None or g.id in selected_ids]

        if selected_goals:
            goals_text = "\n".join([f"- {g.goal} (é‡è¦ç¨‹åº¦: {g.importance})" for g in selected_goals])
            goals_context = f"""## ç”¨æˆ·æƒ³æŒæ¡çš„å­¦ä¹ ç›®æ ‡
{goals_text}"""
            context_parts.append(goals_context)
            source_info.append({"type": "learning_goals", "name": "å­¦ä¹ ç›®æ ‡"})

    # 4. ğŸ¯ ç”¨æˆ·å­¦ä¹ ç”»åƒï¼ˆä¸ªæ€§åŒ–å‡ºé¢˜æ ¸å¿ƒï¼‰
    if request.learner_profile_prompt:
        context_parts.append(f"""---

{request.learner_profile_prompt}

**é‡è¦ï¼šè¯·æ ¹æ®ä¸Šè¿°ç”¨æˆ·ç”»åƒï¼Œä¼˜å…ˆé’ˆå¯¹è–„å¼±æ¦‚å¿µå‡ºé¢˜ï¼Œé¿å…é‡å¤è€ƒå¯Ÿå·²æŒæ¡çš„å†…å®¹ã€‚**""")
        source_info.append({"type": "learner_profile", "name": "ç”¨æˆ·å­¦ä¹ ç”»åƒ"})

    # å…¼å®¹æ—§çš„ materials_content å­—æ®µ
    if not context_parts and request.materials_content:
        context_parts.append(f"## å‚è€ƒèµ„æ–™\n{request.materials_content[:3000]}")

    return "\n\n---\n\n".join(context_parts), source_info


# ==================== V3 ä¸¤é˜¶æ®µå¹¶è¡Œç­–ç•¥ï¼ˆå®˜æ–¹ API ä¸“ç”¨ï¼‰====================

class QuestionPlannerAgent:
    """è§„åˆ’ Agent - è§„åˆ’é¢˜ç›®åˆ†å¸ƒï¼ˆé¢˜å‹ã€éš¾åº¦ã€è€ƒç‚¹ã€æ¥æºï¼‰"""

    SYSTEM_PROMPT = """ä½ æ˜¯æ•™è‚²æµ‹è¯„è§„åˆ’ä¸“å®¶ã€‚æ ¹æ®å­¦ä¹ å†…å®¹ï¼Œè§„åˆ’ä¸€ç»„ç»ƒä¹ é¢˜çš„åˆ†å¸ƒã€‚

è¦æ±‚ï¼š
- æ ¹æ®éš¾åº¦åˆ†å¸ƒé…ç½®åˆ†é…é¢˜ç›®éš¾åº¦
- æ¯é“é¢˜æœ‰æ˜ç¡®çš„è€ƒå¯Ÿé‡ç‚¹å’Œæ¥æº
- è€ƒå¯Ÿç‚¹è¦è¦†ç›–æä¾›çš„å„ç±»å­¦ä¹ å†…å®¹

è¿”å› JSON:
{"plans":[{"index":0,"type":"single","difficulty":"easy","focusPoint":"è€ƒå¯Ÿé‡ç‚¹","questionDirection":"å‡ºé¢˜æ–¹å‘","sourceType":"node_content|user_material|learning_goals","sourceName":"æ¥æºåç§°"}]}

type åªèƒ½æ˜¯: single(å•é€‰), judge(åˆ¤æ–­)
difficulty åªèƒ½æ˜¯: easy, medium, hard"""

    def __init__(self, client: LLMClient):
        self.client = client
        self.model = "qwen-plus"

    async def plan(self, request: QuestionBatchRequest, context: str, source_info: list[dict]) -> list[QuestionPlanItem]:
        # è®¡ç®—éš¾åº¦åˆ†å¸ƒ
        easy_count = round(request.count * request.difficulty.easy / 100)
        medium_count = round(request.count * request.difficulty.medium / 100)
        hard_count = request.count - easy_count - medium_count

        mode_label = {"normal": "ç»¼åˆç»ƒä¹ ", "review": "å¤ä¹ å·©å›º", "advance": "è¿›é˜¶æŒ‘æˆ˜"}.get(request.mode, "ç»¼åˆç»ƒä¹ ")

        user_prompt = f"""## å­¦ä¹ å†…å®¹
{context[:4000]}

## å‡ºé¢˜é…ç½®
- ç”Ÿæˆæ•°é‡ï¼š{request.count} é“é¢˜ç›®
- å…è®¸é¢˜å‹ï¼š{', '.join(request.question_types)}
- å‡ºé¢˜æ¨¡å¼ï¼š{mode_label}
- éš¾åº¦åˆ†å¸ƒï¼šç®€å• {easy_count} é“ï¼Œä¸­ç­‰ {medium_count} é“ï¼Œå›°éš¾ {hard_count} é“
- å¯ç”¨æ¥æºï¼š{', '.join([s.get('name', '') for s in source_info])}

è¯·è§„åˆ’é¢˜ç›®åˆ†å¸ƒï¼Œç¡®ä¿è€ƒå¯Ÿç‚¹è¦†ç›–å„ä¸ªæ¥æºçš„æ ¸å¿ƒå†…å®¹ã€‚"""

        messages = [
            {"role": "system", "content": self.SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ]

        result = await self.client.chat(self.model, messages, max_tokens=1500, temperature=0.3)
        parsed = _extract_json(result)

        if not parsed or not isinstance(parsed, dict):
            return self._default_plan(request, source_info)

        plans = parsed.get("plans", [])
        if not plans:
            return self._default_plan(request, source_info)

        return [
            QuestionPlanItem(
                index=p.get("index", i),
                type=p.get("type", "single") if p.get("type") in ["single", "judge"] else "single",
                difficulty=p.get("difficulty", "medium") if p.get("difficulty") in ["easy", "medium", "hard"] else "medium",
                focus_point=p.get("focusPoint", request.node_name),
                question_direction=p.get("questionDirection", f"è€ƒå¯Ÿ{request.node_name}"),
                source_type=p.get("sourceType", "node_content"),
                source_name=p.get("sourceName", request.node_name),
            )
            for i, p in enumerate(plans[:request.count])
        ]

    def _default_plan(self, request: QuestionBatchRequest, source_info: list[dict]) -> list[QuestionPlanItem]:
        """ç”Ÿæˆé»˜è®¤è§„åˆ’"""
        plans = []
        # æ ¹æ®éš¾åº¦åˆ†å¸ƒè®¡ç®—
        easy_count = round(request.count * request.difficulty.easy / 100)
        medium_count = round(request.count * request.difficulty.medium / 100)

        for i in range(request.count):
            if i < easy_count:
                diff = "easy"
            elif i < easy_count + medium_count:
                diff = "medium"
            else:
                diff = "hard"

            # è½®æµä½¿ç”¨ä¸åŒæ¥æº
            source = source_info[i % len(source_info)] if source_info else {"type": "node_content", "name": request.node_name}

            plans.append(QuestionPlanItem(
                index=i,
                type="single" if i % 2 == 0 else "judge",
                difficulty=diff,
                focus_point=request.key_concepts[i % len(request.key_concepts)] if request.key_concepts else request.node_name,
                question_direction=f"è€ƒå¯Ÿ{request.node_name}çš„ç†è§£",
                source_type=source.get("type", "node_content"),
                source_name=source.get("name", request.node_name),
            ))
        return plans


class QuestionGeneratorAgent:
    """å†…å®¹ç”Ÿæˆ Agent - å¹¶å‘ç”Ÿæˆå•é“é¢˜ç›®ï¼ˆå¸¦å®Œæ•´ AI å…ƒæ•°æ®ï¼‰"""

    SYSTEM_PROMPT = """ä½ æ˜¯ä¸“ä¸šçš„æ•™è‚²æµ‹è¯„ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®å­¦ç”Ÿçš„å­¦ä¹ æƒ…å†µè¿›è¡Œä¸ªæ€§åŒ–å‡ºé¢˜ã€‚

## é¢˜å‹è¯´æ˜
- single: å•é€‰é¢˜ï¼Œ4ä¸ªé€‰é¡¹ï¼Œåªæœ‰1ä¸ªæ­£ç¡®ç­”æ¡ˆ
- judge: åˆ¤æ–­é¢˜ï¼Œé€‰é¡¹ä¸º ["A. æ­£ç¡®", "B. é”™è¯¯"]

## æ•°å­¦å…¬å¼æ ¼å¼
- ä½¿ç”¨ LaTeX æ ¼å¼ï¼Œè¡Œå†…å…¬å¼ç”¨ $...$

## ä¸ªæ€§åŒ–å‡ºé¢˜åŸåˆ™
- å¦‚æœæœ‰ç”¨æˆ·ç”»åƒï¼Œä¼˜å…ˆé’ˆå¯¹è–„å¼±æ¦‚å¿µå‡ºé¢˜
- å¯¹å·²æŒæ¡çš„æ¦‚å¿µå¯ä»¥æé«˜éš¾åº¦æˆ–å‡å°‘å‡ºé¢˜
- æ ¹æ®ç”¨æˆ·çš„é”™è¯¯æ¨¡å¼è®¾è®¡é’ˆå¯¹æ€§é¢˜ç›®

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼ JSONï¼ŒåŒ…å«å®Œæ•´å…ƒæ•°æ®ï¼‰
{
  "type": "single",
  "content": "é¢˜ç›®å†…å®¹",
  "options": ["A. é€‰é¡¹1", "B. é€‰é¡¹2", "C. é€‰é¡¹3", "D. é€‰é¡¹4"],
  "answer": "A",
  "explanation": "ã€ç­”æ¡ˆè§£æã€‘...\\nã€æ˜“é”™åˆ†æã€‘...",
  "difficulty": "easy/medium/hard",
  "difficultyReason": "éš¾åº¦åˆ¤å®šåŸå› ",
  "trace": {
    "sourceContext": "è¿™é“é¢˜åŸºäºå“ªéƒ¨åˆ†å†…å®¹å‡ºé¢˜",
    "confidence": 0.9
  },
  "tags": {
    "concepts": ["æ ¸å¿ƒæ¦‚å¿µ1", "æ ¸å¿ƒæ¦‚å¿µ2"],
    "skills": ["è€ƒå¯Ÿçš„èƒ½åŠ›"],
    "bloomLevel": "understand/apply/analyze",
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
    "similarQuestionHints": ["å¯ä»¥ä»XXè§’åº¦å‡ºå˜å¼é¢˜", "å¯ä»¥è€ƒå¯ŸYY"]
  },
  "variationHints": ["å˜å¼1ï¼šæ”¹å˜æ¡ä»¶...", "å˜å¼2ï¼šåå‘æé—®..."],
  "targetWeakness": "é’ˆå¯¹çš„è–„å¼±ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰"
}

åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

    PROMPT_VERSION = "v3.2"  # ç”¨äºè¿½è¸ª prompt è¿­ä»£ï¼ˆåŠ å…¥ä¸ªæ€§åŒ–å‡ºé¢˜ï¼‰

    def __init__(self, client: LLMClient, max_concurrent: int = 8):
        self.client = client
        self.model = "qwen-max"
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.generation_id = str(uuid.uuid4())[:8]  # æœ¬æ‰¹æ¬¡ç”Ÿæˆ ID

    async def generate_question(
        self,
        plan: QuestionPlanItem,
        request: QuestionBatchRequest,
        context: str,
    ) -> QuestionSchema:
        """ç”Ÿæˆå•é“é¢˜ç›®ï¼ˆå¸¦é™æµå’Œå®Œæ•´å…ƒæ•°æ®ï¼‰"""
        async with self.semaphore:
            mode_hint = {
                "review": "å¤ä¹ å·©å›ºæ¨¡å¼ï¼šåŸºç¡€ä¼˜å…ˆã€æ˜“é”™ç‚¹ä¼˜å…ˆã€é¿å…è¿‡åº¦åˆé’»",
                "advance": "è¿›é˜¶æŒ‘æˆ˜æ¨¡å¼ï¼šç»¼åˆåº”ç”¨ã€å¤šæ­¥éª¤æ¨ç†ã€å¯åŒ…å«å˜å¼é¢˜",
                "normal": "ç»¼åˆç»ƒä¹ æ¨¡å¼ï¼šè¦†ç›–æ ¸å¿ƒçŸ¥è¯†ç‚¹",
            }.get(request.mode, "")

            user_prompt = f"""## å­¦ä¹ å†…å®¹
{context[:3000]}

## å‡ºé¢˜è¦æ±‚
- é¢˜å‹ï¼š{plan.type}
- éš¾åº¦ï¼š{plan.difficulty}
- è€ƒå¯Ÿé‡ç‚¹ï¼š{plan.focus_point}
- å‡ºé¢˜æ–¹å‘ï¼š{plan.question_direction}
- æ¥æºï¼š{plan.source_name}
- æ¨¡å¼æç¤ºï¼š{mode_hint}

è¯·åŸºäºä¸Šè¿°å­¦ä¹ å†…å®¹ï¼Œç”Ÿæˆä¸€é“ç¬¦åˆè¦æ±‚çš„é¢˜ç›®ã€‚
è¦æ±‚ï¼š
1. é¢˜ç›®å¿…é¡»æœ‰æ˜ç¡®ä¾æ®ï¼Œä¸è¦å‡­ç©ºç¼–é€ 
2. å¡«å†™å®Œæ•´çš„ tags ä¿¡æ¯ï¼Œä¾¿äºåç»­æ£€ç´¢
3. æä¾› variationHintsï¼Œä¾¿äºç”Ÿæˆå˜å¼é¢˜"""

            messages = [
                {"role": "system", "content": self.SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ]

            try:
                result = await self.client.chat(self.model, messages, max_tokens=1200, temperature=0.7)
                parsed = _extract_json(result)

                if parsed and isinstance(parsed, dict):
                    return self._build_question_from_response(parsed, plan, request)
            except Exception as e:
                print(f"[QuestionGenerator] ç”Ÿæˆå¤±è´¥: {e}")

            return self._default_question(plan, request)

    def _build_question_from_response(
        self,
        parsed: dict,
        plan: QuestionPlanItem,
        request: QuestionBatchRequest,
    ) -> QuestionSchema:
        """ä» AI å“åº”æ„å»ºå®Œæ•´çš„ QuestionSchema"""
        # è§£æ trace
        trace_data = parsed.get("trace", {})
        trace = AIContentTrace(
            source_type=plan.source_type,
            source_id=trace_data.get("sourceId", ""),
            source_name=plan.source_name,
            source_context=trace_data.get("sourceContext", parsed.get("sourceContext", "")),
            confidence=trace_data.get("confidence", 0.8),
        )

        # è§£æ tags
        tags_data = parsed.get("tags", {})
        tags = AISearchableTags(
            concepts=tags_data.get("concepts", parsed.get("relatedConcepts", [])),
            skills=tags_data.get("skills", []),
            bloom_level=tags_data.get("bloomLevel", "understand"),
            keywords=tags_data.get("keywords", []),
            similar_question_hints=tags_data.get("similarQuestionHints", []),
        )

        # æ„å»º AI å…ƒæ•°æ®
        ai_meta = AIGenerationMeta(
            model_id=self.model,
            strategy="parallel",
            api_mode="system",
            prompt_version=self.PROMPT_VERSION,
            generation_id=self.generation_id,
        )

        return QuestionSchema(
            id=str(uuid.uuid4()),
            type=parsed.get("type", plan.type),
            difficulty=parsed.get("difficulty", plan.difficulty),
            content=parsed.get("content", ""),
            options=parsed.get("options", []),
            answer=parsed.get("answer", ""),
            explanation=parsed.get("explanation", ""),
            trace=trace,
            tags=tags,
            ai_meta=ai_meta,
            difficulty_reason=parsed.get("difficultyReason", ""),
            target_goal_name=plan.focus_point,
            variation_hints=parsed.get("variationHints", []),
        )

    def _default_question(self, plan: QuestionPlanItem, request: QuestionBatchRequest) -> QuestionSchema:
        """ç”Ÿæˆé»˜è®¤é¢˜ç›®ï¼ˆå¸¦å®Œæ•´å…ƒæ•°æ®ï¼‰"""
        # æ„å»ºé»˜è®¤å…ƒæ•°æ®
        trace = AIContentTrace(
            source_type=plan.source_type,
            source_name=plan.source_name,
            source_context="é»˜è®¤ç”Ÿæˆ",
            confidence=0.5,
        )
        tags = AISearchableTags(
            concepts=[plan.focus_point],
            keywords=[request.node_name, plan.focus_point],
        )
        ai_meta = AIGenerationMeta(
            model_id=self.model,
            strategy="parallel",
            api_mode="system",
            prompt_version=self.PROMPT_VERSION,
            generation_id=self.generation_id,
        )

        if plan.type == "judge":
            return QuestionSchema(
                id=str(uuid.uuid4()),
                type="judge",
                difficulty=plan.difficulty,
                content=f"åˆ¤æ–­ï¼šå…³äº{request.node_name}ï¼Œ{plan.focus_point}çš„è¯´æ³•æ˜¯æ­£ç¡®çš„ã€‚",
                options=["A. æ­£ç¡®", "B. é”™è¯¯"],
                answer="A",
                explanation=f"è¿™é“é¢˜è€ƒå¯Ÿ{plan.focus_point}çš„ç†è§£ã€‚",
                trace=trace,
                tags=tags,
                ai_meta=ai_meta,
                target_goal_name=plan.focus_point,
            )
        return QuestionSchema(
            id=str(uuid.uuid4()),
            type="single",
            difficulty=plan.difficulty,
            content=f"å…³äº{request.node_name}ä¸­çš„{plan.focus_point}ï¼Œä»¥ä¸‹è¯´æ³•æ­£ç¡®çš„æ˜¯ï¼Ÿ",
            options=["A. é€‰é¡¹A", "B. é€‰é¡¹B", "C. é€‰é¡¹C", "D. é€‰é¡¹D"],
            answer="A",
            explanation=f"è¿™é“é¢˜è€ƒå¯Ÿ{plan.focus_point}çš„ç†è§£ã€‚",
            trace=trace,
            tags=tags,
            ai_meta=ai_meta,
            target_goal_name=plan.focus_point,
        )



# ==================== å¹¶è¡Œç”Ÿæˆä¸»å‡½æ•° ====================

async def generate_questions_parallel(
    client: LLMClient,
    request: QuestionBatchRequest,
) -> list[QuestionSchema]:
    """V3 ä¸¤é˜¶æ®µå¹¶è¡Œç­–ç•¥ç”Ÿæˆé¢˜ç›®"""
    planner = QuestionPlannerAgent(client)
    generator = QuestionGeneratorAgent(client)

    # æ„å»ºä¸Šä¸‹æ–‡
    context, source_info = _build_context_from_request(request)

    # é˜¶æ®µ1: è§„åˆ’é¢˜ç›®åˆ†å¸ƒ
    plans = await planner.plan(request, context, source_info)

    # é˜¶æ®µ2: å¹¶å‘ç”Ÿæˆæ¯é“é¢˜ç›®
    async def generate_one(plan: QuestionPlanItem) -> tuple[int, QuestionSchema]:
        question = await generator.generate_question(plan, request, context)
        return plan.index, question

    tasks = [generate_one(plan) for plan in plans]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # ç»„è£…ç»“æœï¼ˆæŒ‰é¡ºåºï¼‰
    questions_map = {}
    for result in results:
        if isinstance(result, tuple):
            idx, question = result
            questions_map[idx] = question

    return [questions_map[i] for i in sorted(questions_map.keys())]


async def generate_questions_parallel_stream(
    client: LLMClient,
    request: QuestionBatchRequest,
) -> AsyncGenerator[QuestionProgressEvent | list[QuestionSchema], None]:
    """V3 ä¸¤é˜¶æ®µå¹¶è¡Œç­–ç•¥ - æµå¼ç‰ˆæœ¬"""
    planner = QuestionPlannerAgent(client)
    generator = QuestionGeneratorAgent(client)

    # æ„å»ºä¸Šä¸‹æ–‡
    context, source_info = _build_context_from_request(request)

    # é˜¶æ®µ1: è§„åˆ’
    yield QuestionProgressEvent(
        stage="planning",
        progress=5,
        message="æ­£åœ¨è§„åˆ’é¢˜ç›®åˆ†å¸ƒ...",
        total_questions=request.count,
    )

    plans = await planner.plan(request, context, source_info)
    total = len(plans)

    yield QuestionProgressEvent(
        stage="planning",
        progress=15,
        message=f"è§„åˆ’å®Œæˆï¼Œå…± {total} é“é¢˜ç›®",
        total_questions=total,
    )

    # é˜¶æ®µ2: å¹¶å‘ç”Ÿæˆï¼ˆå¸¦è¿›åº¦ï¼‰
    completed = 0
    results = {}
    lock = asyncio.Lock()

    async def generate_with_progress(plan: QuestionPlanItem) -> tuple[int, QuestionSchema]:
        nonlocal completed
        question = await generator.generate_question(plan, request, context)
        async with lock:
            completed += 1
            results[plan.index] = question
        return plan.index, question

    tasks = [generate_with_progress(plan) for plan in plans]

    for coro in asyncio.as_completed(tasks):
        try:
            idx, _ = await coro
            progress = 15 + (completed / total) * 80
            yield QuestionProgressEvent(
                stage="generating",
                progress=progress,
                message=f"å·²ç”Ÿæˆç¬¬ {completed}/{total} é“é¢˜ç›®",
                current_question=idx + 1,
                total_questions=total,
                completed_questions=completed,
            )
        except Exception as e:
            print(f"[QuestionStream] ç”Ÿæˆå¤±è´¥: {e}")

    yield QuestionProgressEvent(
        stage="done",
        progress=100,
        message="é¢˜ç›®ç”Ÿæˆå®Œæˆ",
        total_questions=total,
        completed_questions=total,
    )

    # è¿”å›æ’åºåçš„é¢˜ç›®åˆ—è¡¨
    yield [results[i] for i in sorted(results.keys())]


# ==================== å•æ¬¡ç”Ÿæˆç­–ç•¥ï¼ˆç”¨æˆ·è‡ªé… APIï¼‰====================

SINGLE_REQUEST_PROMPT = """ä½ æ˜¯ä¸“ä¸šçš„æ•™è‚²æµ‹è¯„ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®å­¦ç”Ÿçš„å­¦ä¹ æƒ…å†µè¿›è¡Œä¸ªæ€§åŒ–å‡ºé¢˜ã€‚

## é¢˜å‹è¯´æ˜
- single: å•é€‰é¢˜ï¼Œ4ä¸ªé€‰é¡¹ï¼Œåªæœ‰1ä¸ªæ­£ç¡®ç­”æ¡ˆ
- judge: åˆ¤æ–­é¢˜ï¼Œé€‰é¡¹ä¸º ["A. æ­£ç¡®", "B. é”™è¯¯"]

## æ•°å­¦å…¬å¼æ ¼å¼
- ä½¿ç”¨ LaTeX æ ¼å¼ï¼Œè¡Œå†…å…¬å¼ç”¨ $...$

## ä¸ªæ€§åŒ–å‡ºé¢˜åŸåˆ™
- å¦‚æœæœ‰ç”¨æˆ·ç”»åƒï¼Œä¼˜å…ˆé’ˆå¯¹è–„å¼±æ¦‚å¿µå‡ºé¢˜ï¼ˆçº¦70%ï¼‰
- å¯¹å·²æŒæ¡çš„æ¦‚å¿µå¯ä»¥æé«˜éš¾åº¦æˆ–å‡å°‘å‡ºé¢˜
- æ ¹æ®ç”¨æˆ·çš„é”™è¯¯æ¨¡å¼è®¾è®¡é’ˆå¯¹æ€§é¢˜ç›®

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONæ•°ç»„ï¼ŒåŒ…å«å®Œæ•´å…ƒæ•°æ®ï¼‰
[
  {
    "type": "single",
    "content": "é¢˜ç›®å†…å®¹",
    "options": ["A. é€‰é¡¹1", "B. é€‰é¡¹2", "C. é€‰é¡¹3", "D. é€‰é¡¹4"],
    "answer": "A",
    "explanation": "ç­”æ¡ˆè§£æ",
    "difficulty": "easy|medium|hard",
    "difficultyReason": "éš¾åº¦åˆ¤å®šåŸå› ",
    "trace": {
      "sourceContext": "å‡ºé¢˜ä¾æ®",
      "confidence": 0.9
    },
    "tags": {
      "concepts": ["æ ¸å¿ƒæ¦‚å¿µ"],
      "skills": ["è€ƒå¯Ÿèƒ½åŠ›"],
      "bloomLevel": "understand",
      "keywords": ["å…³é”®è¯"]
    },
    "variationHints": ["å˜å¼é¢˜æç¤º"],
    "targetWeakness": "é’ˆå¯¹çš„è–„å¼±ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰"
  }
]

åªè¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

SINGLE_REQUEST_PROMPT_VERSION = "v3.2"


async def generate_questions_single_request(
    client: LLMClient,
    request: QuestionBatchRequest,
    model_id: str,
) -> list[QuestionSchema]:
    """å•æ¬¡è¯·æ±‚ç­–ç•¥ï¼ˆç”¨æˆ·è‡ªé… APIï¼‰- å¸¦å®Œæ•´ AI å…ƒæ•°æ®"""
    # æ„å»ºä¸Šä¸‹æ–‡
    context, source_info = _build_context_from_request(request)
    generation_id = str(uuid.uuid4())[:8]

    mode_label = {"normal": "ç»¼åˆç»ƒä¹ ", "review": "å¤ä¹ å·©å›º", "advance": "è¿›é˜¶æŒ‘æˆ˜"}.get(request.mode, "ç»¼åˆç»ƒä¹ ")

    # è®¡ç®—éš¾åº¦åˆ†å¸ƒ
    easy_count = round(request.count * request.difficulty.easy / 100)
    medium_count = round(request.count * request.difficulty.medium / 100)
    hard_count = request.count - easy_count - medium_count

    user_prompt = f"""## å­¦ä¹ å†…å®¹
{context[:5000]}

## å‡ºé¢˜è¦æ±‚
- ç”Ÿæˆæ•°é‡ï¼š{request.count} é“é¢˜ç›®
- é¢˜å‹åŒ…æ‹¬ï¼š{', '.join(request.question_types)}
- å‡ºé¢˜æ¨¡å¼ï¼š{mode_label}
- éš¾åº¦åˆ†å¸ƒï¼šç®€å• {easy_count} é“ï¼Œä¸­ç­‰ {medium_count} é“ï¼Œå›°éš¾ {hard_count} é“

è¯·åŸºäºä¸Šè¿°å­¦ä¹ å†…å®¹ç”Ÿæˆé¢˜ç›®ï¼š
1. æ¯é“é¢˜å¿…é¡»æœ‰æ˜ç¡®ä¾æ®ï¼Œä¸è¦å‡­ç©ºç¼–é€ 
2. å¡«å†™å®Œæ•´çš„ tags ä¿¡æ¯ï¼Œä¾¿äºåç»­æ£€ç´¢
3. æä¾› variationHintsï¼Œä¾¿äºç”Ÿæˆå˜å¼é¢˜"""

    messages = [
        {"role": "system", "content": SINGLE_REQUEST_PROMPT},
        {"role": "user", "content": user_prompt},
    ]

    result = await client.chat(model_id, messages, max_tokens=8000, temperature=0.7)
    parsed = _extract_json(result)

    if not parsed or not isinstance(parsed, list):
        return []

    questions = []
    for i, q in enumerate(parsed[:request.count]):
        # è§£æ trace
        trace_data = q.get("trace", {})
        source = source_info[i % len(source_info)] if source_info else {"type": "node_content", "name": request.node_name}
        trace = AIContentTrace(
            source_type=source.get("type", "node_content"),
            source_id=source.get("id", ""),
            source_name=source.get("name", request.node_name),
            source_context=trace_data.get("sourceContext", q.get("sourceContext", "")),
            confidence=trace_data.get("confidence", 0.8),
        )

        # è§£æ tags
        tags_data = q.get("tags", {})
        tags = AISearchableTags(
            concepts=tags_data.get("concepts", q.get("relatedConcepts", [])),
            skills=tags_data.get("skills", []),
            bloom_level=tags_data.get("bloomLevel", "understand"),
            keywords=tags_data.get("keywords", []),
            similar_question_hints=tags_data.get("similarQuestionHints", []),
        )

        # æ„å»º AI å…ƒæ•°æ®
        ai_meta = AIGenerationMeta(
            model_id=model_id,
            strategy="single",
            api_mode="byok",
            prompt_version=SINGLE_REQUEST_PROMPT_VERSION,
            generation_id=generation_id,
        )

        questions.append(QuestionSchema(
            id=str(uuid.uuid4()),
            type=q.get("type", "single"),
            difficulty=q.get("difficulty", "medium"),
            content=q.get("content", ""),
            options=q.get("options", []),
            answer=q.get("answer", ""),
            explanation=q.get("explanation", ""),
            trace=trace,
            tags=tags,
            ai_meta=ai_meta,
            difficulty_reason=q.get("difficultyReason", ""),
            target_goal_name=q.get("targetGoalName"),
            variation_hints=q.get("variationHints", []),
        ))

    return questions


# ==================== ä¸»å…¥å£ ====================

async def generate_questions_batch(
    db: Session,
    user_id: str,
    request: QuestionBatchRequest,
    use_system: bool | None = None,
) -> QuestionGenerationResult:
    """
    æ‰¹é‡ç”Ÿæˆé¢˜ç›®

    Args:
        db: æ•°æ®åº“ä¼šè¯
        user_id: ç”¨æˆ· ID
        request: æ‰¹é‡ç”Ÿæˆè¯·æ±‚
        use_system: æ˜¯å¦ä½¿ç”¨ç³»ç»Ÿ APIï¼ˆNone è¡¨ç¤ºè‡ªåŠ¨åˆ¤æ–­ï¼‰

    Returns:
        QuestionGenerationResult: ç”Ÿæˆç»“æœ
    """
    start_time = time.time()

    try:
        resolved = resolve_llm_config(
            db,
            user_id=user_id,
            requested_use_system=use_system,
            override_api_key=None,
            override_base_url=None,
            override_model_id=None,
            override_routing=None,
        )
    except Exception as e:
        return QuestionGenerationResult(
            success=False,
            error=str(e),
            mode="byok",
            strategy="single_request",
            processing_time_ms=(time.time() - start_time) * 1000,
            model_used="",
        )

    client = LLMClient(resolved.api_key, resolved.base_url)

    if resolved.mode == "system":
        # å®˜æ–¹ API: ä½¿ç”¨ä¸¤é˜¶æ®µå¹¶è¡Œç­–ç•¥
        try:
            questions = await generate_questions_parallel(client, request)
            return QuestionGenerationResult(
                success=True,
                questions=questions,
                mode="system",
                strategy="parallel",
                processing_time_ms=(time.time() - start_time) * 1000,
                model_used="qwen-plus (è§„åˆ’) + qwen-max (ç”Ÿæˆ)",
            )
        except Exception as e:
            return QuestionGenerationResult(
                success=False,
                error=str(e),
                mode="system",
                strategy="parallel",
                processing_time_ms=(time.time() - start_time) * 1000,
                model_used="qwen-plus + qwen-max",
            )
    else:
        # ç”¨æˆ·è‡ªé… API: ä½¿ç”¨å•æ¬¡ç”Ÿæˆç­–ç•¥
        model_id = resolved.model_id or "gpt-4"
        try:
            questions = await generate_questions_single_request(client, request, model_id)
            return QuestionGenerationResult(
                success=True,
                questions=questions,
                mode="byok",
                strategy="single_request",
                processing_time_ms=(time.time() - start_time) * 1000,
                model_used=model_id,
            )
        except Exception as e:
            return QuestionGenerationResult(
                success=False,
                error=str(e),
                mode="byok",
                strategy="single_request",
                processing_time_ms=(time.time() - start_time) * 1000,
                model_used=model_id,
            )



async def generate_questions_batch_stream(
    db: Session,
    user_id: str,
    request: QuestionBatchRequest,
    use_system: bool | None = None,
) -> AsyncGenerator[QuestionProgressEvent | QuestionGenerationResult, None]:
    """
    æµå¼æ‰¹é‡ç”Ÿæˆé¢˜ç›®ï¼ˆå¸¦è¿›åº¦åé¦ˆï¼‰
    """
    start_time = time.time()

    try:
        resolved = resolve_llm_config(
            db,
            user_id=user_id,
            requested_use_system=use_system,
            override_api_key=None,
            override_base_url=None,
            override_model_id=None,
            override_routing=None,
        )
    except Exception as e:
        yield QuestionGenerationResult(
            success=False,
            error=str(e),
            mode="byok",
            strategy="single_request",
            processing_time_ms=(time.time() - start_time) * 1000,
            model_used="",
        )
        return

    client = LLMClient(resolved.api_key, resolved.base_url)

    if resolved.mode == "system":
        # å®˜æ–¹ API: ä½¿ç”¨æµå¼å¹¶è¡Œç­–ç•¥
        try:
            questions = []
            async for event in generate_questions_parallel_stream(client, request):
                if isinstance(event, QuestionProgressEvent):
                    yield event
                elif isinstance(event, list):
                    questions = event

            yield QuestionGenerationResult(
                success=True,
                questions=questions,
                mode="system",
                strategy="parallel",
                processing_time_ms=(time.time() - start_time) * 1000,
                model_used="qwen-plus (è§„åˆ’) + qwen-max (ç”Ÿæˆ)",
            )
        except Exception as e:
            yield QuestionGenerationResult(
                success=False,
                error=str(e),
                mode="system",
                strategy="parallel",
                processing_time_ms=(time.time() - start_time) * 1000,
                model_used="qwen-plus + qwen-max",
            )
    else:
        # ç”¨æˆ·è‡ªé… API: å•æ¬¡è¯·æ±‚ï¼ˆæ— æµå¼ï¼‰
        model_id = resolved.model_id or "gpt-4"
        yield QuestionProgressEvent(
            stage="generating",
            progress=10,
            message="æ­£åœ¨ç”Ÿæˆé¢˜ç›®...",
            total_questions=request.count,
        )
        try:
            questions = await generate_questions_single_request(client, request, model_id)
            yield QuestionProgressEvent(
                stage="done",
                progress=100,
                message="é¢˜ç›®ç”Ÿæˆå®Œæˆ",
                total_questions=len(questions),
                completed_questions=len(questions),
            )
            yield QuestionGenerationResult(
                success=True,
                questions=questions,
                mode="byok",
                strategy="single_request",
                processing_time_ms=(time.time() - start_time) * 1000,
                model_used=model_id,
            )
        except Exception as e:
            yield QuestionGenerationResult(
                success=False,
                error=str(e),
                mode="byok",
                strategy="single_request",
                processing_time_ms=(time.time() - start_time) * 1000,
                model_used=model_id,
            )